{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 17:57:07.522789: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751997427.540543    9743 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751997427.546001    9743 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751997427.559553    9743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751997427.559567    9743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751997427.559570    9743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751997427.559572    9743 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-08 17:57:07.564809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/root/projects/prec/prec-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ! pip install optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from collections import deque\n",
    "import random\n",
    "import optuna\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from typing import Tuple, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class BTCDataset:\n",
    "    def __init__(self, dataset_path='datasets/structured_dataset.csv', lookback=2*12, horizon=12):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.lookback = lookback\n",
    "        self.horizon = horizon\n",
    "        self.scaler = StandardScaler()\n",
    "        self.target_cols = ['close', 'low', 'high']\n",
    "        # self.feature_cols = []  # all features are used\n",
    "        # self.X, self.y = self.load_dataset()\n",
    "\n",
    "    def load_dataset(self):\n",
    "        df = pd.read_csv(self.dataset_path, parse_dates=['timestamp'])\n",
    "        # print(f\"Initial columns: {df.columns.tolist()}\")\n",
    "        \n",
    "        # First create targets (this adds target columns)\n",
    "        # print(f\"Creating targets...\")\n",
    "        # df = self.create_targets(df)\n",
    "        # # print(f\"After create_targets: {df.columns.tolist()}\")\n",
    "        # df_with_targets = df.copy()  # Save version with targets\n",
    "        \n",
    "        # Then clean the data\n",
    "        print(f\"Cleaning data...\")\n",
    "        df = self.clean_data(df)\n",
    "        # print(f\"After clean_data: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Then add additional features\n",
    "        print(f\"Adding features...\")\n",
    "        df = self.add_features(df)\n",
    "        # print(f\"After add_features: {df.columns.tolist()}\")\n",
    "        # --- FIX: Add target columns back if missing ---\n",
    "        # for col in self.target_cols:\n",
    "        #     if col in df_with_targets.columns and col not in df.columns:\n",
    "        #         df[col] = df_with_targets[col]\n",
    "        # print(f\"After restoring targets: {df.columns.tolist()}\")\n",
    "        \n",
    "        # Now identify feature columns (all columns except targets)\n",
    "        # self.feature_cols = [col for col in df.columns if col not in self.target_cols]\n",
    "        # print(f\"Feature columns: {self.feature_cols}\")\n",
    "        # print(f\"Target columns: {self.target_cols}\")\n",
    "        \n",
    "        # Finally create sequences\n",
    "        print(f\"Creating sequences...\")\n",
    "        X, y = self.create_sequences(df)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def create_sequences(self, df):\n",
    "        \"\"\"Create time series sequences\"\"\"\n",
    "        X, y = [], []\n",
    "        \n",
    "        # Ensure we don't go out of bounds\n",
    "        max_i = len(df) - self.lookback - self.horizon\n",
    "        \n",
    "        bias = 15000\n",
    "\n",
    "        for i in range(bias + self.lookback, bias + 1000):\n",
    "            # Features (lookback window)\n",
    "            X.append(df[:].iloc[i-self.lookback:i].values)\n",
    "            \n",
    "            # Targets (horizon steps ahead)\n",
    "            y.append(df[self.target_cols].iloc[i+self.horizon-1].values)  # Using the last point in horizon\n",
    "        \n",
    "        return np.array(X), np.array(y)\n",
    "        \n",
    "    def clean_data(self, df):\n",
    "        \"\"\"Handles missing values, outliers, and normalization\"\"\"\n",
    "        # 1. Handle missing values\n",
    "        for col in df.columns:\n",
    "            # Forward fill for technical indicators\n",
    "            if col in ['rsi_14', 'MACD_12_26_9', 'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0', 'obv']:\n",
    "                df[col] = df[col].ffill().bfill()\n",
    "            # Fill with 0 for whale features\n",
    "            elif col.startswith('whale_'):\n",
    "                df[col] = df[col].fillna(0)\n",
    "            # Fill with mean for other numeric features\n",
    "            elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "                df[col] = df[col].fillna(df[col].mean())\n",
    "        \n",
    "        # 2. Remove extreme outliers (99.9th percentile)\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            upper = df[col].quantile(0.999)\n",
    "            lower = df[col].quantile(0.001)\n",
    "            df[col] = np.clip(df[col], lower, upper)\n",
    "\n",
    "        # Check for missing features\n",
    "        missing_features = [col for col in numeric_cols if df[col].isna().any()]\n",
    "        if missing_features:\n",
    "            print(f\"Warning: Missing features: {missing_features}\")\n",
    "            df[missing_features] = df[missing_features].fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        # 3. Normalization - apply scaler only to numeric features that are not time-based\n",
    "        non_temporal_cols = [col for col in numeric_cols \n",
    "                           if not col.endswith(('_sin', '_cos'))]\n",
    "        if len(non_temporal_cols) > 0:\n",
    "            df[non_temporal_cols] = self.scaler.fit_transform(df[non_temporal_cols])\n",
    "        \n",
    "        # Handle infinite values\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        if df.isna().any().any():\n",
    "            raise ValueError(\"NaNs still present after cleaning\")\n",
    "            \n",
    "        return df\n",
    "\n",
    "    # def create_targets(self, df):\n",
    "    #     \"\"\"Create future price targets\"\"\"\n",
    "    #     # 1-hour ahead predictions (12 steps forward)\n",
    "    #     df['future_close'] = df['close'].shift(-self.horizon)\n",
    "    #     df['future_low'] = df['low'].rolling(self.horizon).min().shift(-self.horizon)\n",
    "    #     df['future_high'] = df['high'].rolling(self.horizon).max().shift(-self.horizon)\n",
    "        \n",
    "    #     # Drop rows with NaN targets (end of dataset)\n",
    "    #     df = df.dropna(subset=self.target_cols)\n",
    "    #     return df\n",
    "\n",
    "    def add_features(self, df):\n",
    "        \"\"\"Adds additional predictive features to the dataset\"\"\"\n",
    "        \n",
    "        # Extract time components\n",
    "        df['date'] = pd.to_datetime(df['timestamp'])\n",
    "        df['hour'] = df['date'].dt.hour\n",
    "        df['day_of_week'] = df['date'].dt.dayofweek\n",
    "        \n",
    "        # 1. Price Transformations\n",
    "        # df['log_return'] = np.log(df['close'] / df['close'].shift(1))\n",
    "        \n",
    "        # 2. Time-based Features\n",
    "        df['hour_sin'] = np.sin(df['hour'] * (2 * np.pi / 24))\n",
    "        df['hour_cos'] = np.cos(df['hour'] * (2 * np.pi / 24))\n",
    "        df['day_of_week_sin'] = np.sin(df['day_of_week'] * (2 * np.pi / 7))\n",
    "        df['day_of_week_cos'] = np.cos(df['day_of_week'] * (2 * np.pi / 7))\n",
    "        df['month_sin'] = np.sin(df['date'].dt.month * (2 * np.pi / 12))\n",
    "        df['month_cos'] = np.cos(df['date'].dt.month * (2 * np.pi / 12))\n",
    "        df['day_of_month_sin'] = np.sin(df['date'].dt.day * (2 * np.pi / 30))\n",
    "        df['day_of_month_cos'] = np.cos(df['date'].dt.day * (2 * np.pi / 30))\n",
    "        df['year_sin'] = np.sin(df['date'].dt.year * (2 * np.pi / 4))\n",
    "        df['year_cos'] = np.cos(df['date'].dt.year * (2 * np.pi / 4))\n",
    "\n",
    "        # Define expected features\n",
    "        expected_features = [\n",
    "            'open', 'high', 'low', 'close', 'volume', 'taker_buy_volume',\n",
    "            'whale_tx_count', 'whale_btc_volume',\n",
    "            'rsi_14', 'MACD_12_26_9',\n",
    "            'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0', 'obv', 'vwap',\n",
    "            'hour_sin', 'hour_cos', 'day_of_week_sin', 'day_of_week_cos', \n",
    "            'month_sin', 'month_cos', 'day_of_month_sin', 'day_of_month_cos', \n",
    "            'year_sin', 'year_cos'\n",
    "        ]\n",
    "        \n",
    "        # Check if all features are present\n",
    "        missing_features = [col for col in expected_features if col not in df.columns]\n",
    "        if missing_features:\n",
    "            print(f\"Warning: Missing features: {missing_features}\")\n",
    "            # Add missing features with default values\n",
    "            for col in missing_features:\n",
    "                df[col] = df[col].fillna(method='bfill').fillna(method='ffill')\n",
    "        \n",
    "        # Select only the expected features that exist in the dataset\n",
    "        available_features = [col for col in expected_features if col in df.columns]\n",
    "        df = df[available_features]\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedBitcoinPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=256, num_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Group features into different encoders based on feature types\n",
    "        self.price_encoder = nn.LSTM(4, hidden_size//4, num_layers, batch_first=True)  # open, high, low, close\n",
    "        self.volume_encoder = nn.LSTM(2, hidden_size//8, num_layers, batch_first=True)  # volume, taker_buy_volume\n",
    "        self.whale_encoder = nn.LSTM(2, hidden_size//8, num_layers, batch_first=True)  # whale_tx_count, whale_btc_volume\n",
    "        self.technical_encoder = nn.LSTM(7, hidden_size//2, num_layers, batch_first=True)  # technical indicators\n",
    "        self.temporal_encoder = nn.LSTM(10, hidden_size//4, num_layers, batch_first=True)  # temporal features\n",
    "        \n",
    "        self.concat_dim = hidden_size + hidden_size//4\n",
    "\n",
    "        # Cross-feature attention\n",
    "        self.feature_attention = nn.Sequential(\n",
    "            nn.Linear(self.concat_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 5),  # 5 feature groups\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Prediction heads\n",
    "        self.point_head = nn.Sequential(\n",
    "            nn.Linear(self.concat_dim, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size//2, 1)\n",
    "        )\n",
    "        \n",
    "        self.interval_head = nn.Sequential(\n",
    "            nn.Linear(self.concat_dim, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size//2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size//2, 2),  # min and max\n",
    "            nn.Softplus()  # Ensure positive interval width\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Split input into feature groups based on actual feature order\n",
    "        # Assuming feature order: [price_features, volume_features, whale_features, technical_features, temporal_features]\n",
    "        price_features = x[:, :, :4]  # open, high, low, close\n",
    "        volume_features = x[:, :, 4:6]  # volume, taker_buy_volume\n",
    "        whale_features = x[:, :, 6:8]  # whale_tx_count, whale_btc_volume\n",
    "        tech_features = x[:, :, 8:15]  # technical indicators (rsi, macd, bb, obv, vwap)\n",
    "        temp_features = x[:, :, 15:]  # temporal features\n",
    "        \n",
    "        # Encode each feature group\n",
    "        price_emb, _ = self.price_encoder(price_features)\n",
    "        vol_emb, _ = self.volume_encoder(volume_features)\n",
    "        whale_emb, _ = self.whale_encoder(whale_features)\n",
    "        tech_emb, _ = self.technical_encoder(tech_features)\n",
    "        temp_emb, _ = self.temporal_encoder(temp_features)\n",
    "        \n",
    "        # Concatenate all embeddings\n",
    "        embeddings = torch.cat([\n",
    "            price_emb[:, -1], \n",
    "            vol_emb[:, -1],\n",
    "            whale_emb[:, -1],\n",
    "            tech_emb[:, -1],\n",
    "            temp_emb[:, -1]\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Feature attention weighting\n",
    "        attention_weights = self.feature_attention(embeddings)\n",
    "        weighted_emb = torch.cat([\n",
    "            price_emb[:, -1] * attention_weights[:, 0:1],\n",
    "            vol_emb[:, -1] * attention_weights[:, 1:2],\n",
    "            whale_emb[:, -1] * attention_weights[:, 2:3],\n",
    "            tech_emb[:, -1] * attention_weights[:, 3:4],\n",
    "            temp_emb[:, -1] * attention_weights[:, 4:5]\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Predictions\n",
    "        point_pred = self.point_head(weighted_emb)\n",
    "        interval_pred = self.interval_head(weighted_emb)\n",
    "        \n",
    "        # Ensure min < max in interval\n",
    "        interval_pred = torch.sort(interval_pred, dim=-1)[0]\n",
    "        \n",
    "        return point_pred.squeeze(), interval_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def challenge_loss(point_pred, interval_pred, targets):\n",
    "    \"\"\"\n",
    "    Loss function based on the challenge scoring system:\n",
    "    1. Point prediction error: absolute percentage error\n",
    "    2. Interval prediction: width factor * inclusion factor\n",
    "    \"\"\"\n",
    "    # Extract target values\n",
    "    price_target = targets[:, 0]  # price_1h\n",
    "    low_target = targets[:, 1]    # low_1h\n",
    "    high_target = targets[:, 2]   # high_1h\n",
    "    \n",
    "    # Point prediction loss (absolute percentage error)\n",
    "    point_error = torch.abs(point_pred - price_target) / (price_target + 1e-8)\n",
    "    point_loss = torch.mean(point_error)\n",
    "    \n",
    "    # Interval prediction loss\n",
    "    interval_min, interval_max = interval_pred[:, 0], interval_pred[:, 1]\n",
    "    \n",
    "    # Width factor (normalized interval width)\n",
    "    interval_width = interval_max - interval_min\n",
    "    width_factor = interval_width / (price_target + 1e-8)\n",
    "    \n",
    "    # Inclusion factor (percentage of targets within interval)\n",
    "    within_interval = (low_target >= interval_min) & (high_target <= interval_max)\n",
    "    inclusion_factor = torch.mean(within_interval.float())\n",
    "    \n",
    "    # Interval score (higher is better, so we minimize negative score)\n",
    "    interval_score = inclusion_factor * (1.0 / (1.0 + width_factor))\n",
    "    interval_loss = 1.0 - interval_score\n",
    "    \n",
    "    # Combined loss (weighted sum)\n",
    "    total_loss = 0.5 * point_loss + 0.5 * interval_loss\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_with_cv(X, y, params, save_dir='models', trial_name=None):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # TimeSeriesSplit cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    fold_scores = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRIAL: {trial_name}\")\n",
    "    print(f\"Parameters: {params}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "        print(f\"\\n--- Training fold {fold + 1}/5 ---\")\n",
    "        print(f\"Train samples: {len(train_idx)}, Val samples: {len(val_idx)}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train = torch.FloatTensor(X_train)\n",
    "        y_train = torch.FloatTensor(y_train)\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "        y_val = torch.FloatTensor(y_val)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=params['batch_size'])\n",
    "        \n",
    "        # Initialize model\n",
    "        model = EnhancedBitcoinPredictor(\n",
    "            input_size=X.shape[2],\n",
    "            hidden_size=params['hidden_size'],\n",
    "            num_layers=params['num_layers']\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=params['lr'], \n",
    "                              weight_decay=params['weight_decay'])\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, \n",
    "            max_lr=params['lr'],\n",
    "            steps_per_epoch=len(train_loader),\n",
    "            epochs=params['epochs']\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        best_val_score = float('inf')\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "        \n",
    "        print(f\"Starting training for {params['epochs']} epochs...\")\n",
    "        print(f\"Batch size: {params['batch_size']}, Learning rate: {params['lr']:.6f}\")\n",
    "        \n",
    "        for epoch in range(params['epochs']):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            batch_count = 0\n",
    "            \n",
    "            # Training phase\n",
    "            for batch_idx, (batch_X, batch_y) in enumerate(train_loader):\n",
    "                batch_X = batch_X.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                point_pred, interval_pred = model(batch_X)\n",
    "                \n",
    "                # Use challenge loss function\n",
    "                loss = challenge_loss(point_pred, interval_pred, batch_y)\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Print batch progress every 10 batches\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"  Epoch {epoch+1}/{params['epochs']}, Batch {batch_idx+1}/{len(train_loader)}, \"\n",
    "                          f\"Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Validation\n",
    "            val_score = evaluate(model, val_loader, device)\n",
    "            avg_train_loss = train_loss / batch_count\n",
    "            \n",
    "            # Print epoch progress\n",
    "            print(f\"  Epoch {epoch+1}/{params['epochs']}: \"\n",
    "                  f\"Train Loss = {avg_train_loss:.4f}, Val Score = {val_score:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            early_stopping(val_score)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"  Fold {fold + 1} early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "            \n",
    "            if val_score < best_val_score:\n",
    "                best_val_score = val_score\n",
    "                print(f\"  New best validation score: {best_val_score:.4f}\")\n",
    "                # Save best model for this fold\n",
    "                fold_dir = os.path.join(save_dir, f'fold_{fold + 1}')\n",
    "                os.makedirs(fold_dir, exist_ok=True)\n",
    "                torch.save(model.state_dict(), os.path.join(fold_dir, 'model.pth'))\n",
    "        \n",
    "        fold_scores.append(best_val_score)\n",
    "        print(f\"Fold {fold + 1} completed - Best score: {best_val_score:.4f}\")\n",
    "    \n",
    "    mean_score = np.mean(fold_scores)\n",
    "    std_score = np.std(fold_scores)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRIAL COMPLETED: {trial_name}\")\n",
    "    print(f\"Mean CV Score: {mean_score:.4f} ± {std_score:.4f}\")\n",
    "    print(f\"Individual fold scores: {[f'{score:.4f}' for score in fold_scores]}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in data_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            point_pred, interval_pred = model(batch_X)\n",
    "            \n",
    "            # Challenge-specific loss\n",
    "            loss = challenge_loss(point_pred, interval_pred, batch_y)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = val_score\n",
    "        elif val_score >= self.best_score:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = val_score\n",
    "            self.counter = 0\n",
    "\n",
    "class ContinuousLearner:\n",
    "    def __init__(self, model_path=None, feature_cols=None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if feature_cols is None:\n",
    "            # Default feature count if not provided\n",
    "            input_size = 26  # Based on expected features\n",
    "        else:\n",
    "            input_size = len(feature_cols)\n",
    "            \n",
    "        self.model = EnhancedBitcoinPredictor(input_size=input_size)\n",
    "        \n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        self.buffer = deque(maxlen=1000)\n",
    "        self.steps = 0\n",
    "        self.feature_cols = feature_cols\n",
    "        \n",
    "    def update(self, new_data: pd.DataFrame):\n",
    "        \"\"\"Update model with new data\"\"\"\n",
    "        # Preprocess new data\n",
    "        dataset = BTCDataset()\n",
    "        X, y = dataset.load_dataset()\n",
    "        \n",
    "        # Add to buffer\n",
    "        for i in range(len(X)):\n",
    "            self.buffer.append((\n",
    "                torch.FloatTensor(X[i:i+1]),\n",
    "                torch.FloatTensor(y[i:i+1])\n",
    "            ))\n",
    "        \n",
    "        # Online training step\n",
    "        if len(self.buffer) >= 32:  # Minimum batch size\n",
    "            self.online_train()\n",
    "            \n",
    "    def online_train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = random.sample(self.buffer, min(32, len(self.buffer)))\n",
    "        batch_X = torch.cat([x for x, _ in batch]).to(self.device)\n",
    "        batch_y = torch.cat([y for _, y in batch]).to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        point_pred, interval_pred = self.model(batch_X)\n",
    "        \n",
    "        # Loss calculation\n",
    "        loss = challenge_loss(point_pred, interval_pred, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.steps += 1\n",
    "        \n",
    "        # Periodic validation\n",
    "        if self.steps % 100 == 0:\n",
    "            self.validate()\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate on recent data\"\"\"\n",
    "        if len(self.buffer) < 10:\n",
    "            return\n",
    "            \n",
    "        # Use recent data for validation\n",
    "        val_data = list(self.buffer)[-100:]  # Last 100 samples\n",
    "        val_X = torch.cat([x for x, _ in val_data]).to(self.device)\n",
    "        val_y = torch.cat([y for _, y in val_data]).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            point_pred, interval_pred = self.model(val_X)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            loss = challenge_loss(point_pred, interval_pred, val_y)\n",
    "            \n",
    "        self.model.train()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "            point_pred, interval_pred = self.model(X_tensor)\n",
    "            return point_pred.cpu().numpy(), interval_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial) -> float:\n",
    "    params = {\n",
    "        'hidden_size': trial.suggest_categorical('hidden_size', [128, 256, 512]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 4),\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),\n",
    "        'epochs': 2  # Reduced for faster optimization\n",
    "    }\n",
    "    \n",
    "    trial_name = f\"Trial_{trial.number}\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n{'*'*80}\")\n",
    "        print(f\"STARTING {trial_name}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        print(f\"{'*'*80}\")\n",
    "        \n",
    "        # Load data\n",
    "        print(f\"Loading dataset...\")\n",
    "        try:\n",
    "            dataset = BTCDataset(dataset_path='datasets/structured_dataset.csv')\n",
    "            print(f\"Dataset Found successfully. Loading dataset...\")\n",
    "            X, y = dataset.load_dataset()\n",
    "            \n",
    "            print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "            print(f\"X has {np.isnan(X).sum()} NaN\")\n",
    "            print(f\"y has {np.isnan(y).sum()} NaN\")\n",
    "            assert not np.isnan(X).any(), \"X has NaN\"\n",
    "            assert not np.isnan(y).any(), \"y has NaN\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load dataset: {str(e)}\")\n",
    "            raise\n",
    "        # dataset = BTCDataset(dataset_path='datasets/structured_dataset.csv')\n",
    "        # X, y = dataset.X, dataset.y\n",
    "        # print(f\"Dataset loaded: X shape {X.shape}, y shape {y.shape}\")\n",
    "        \n",
    "        # Train with CV\n",
    "        cv_score = train_with_cv(X, y, params, trial_name=trial_name)\n",
    "        \n",
    "        print(f\"{trial_name} completed successfully with score: {cv_score:.4f}\")\n",
    "        return float(cv_score)\n",
    "    except Exception as e:\n",
    "        print(f\"{trial_name} FAILED with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousLearner:\n",
    "    def __init__(self, model_path=None, feature_cols=None):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        if feature_cols is None:\n",
    "            # Default feature count if not provided\n",
    "            input_size = 26  # Based on expected features\n",
    "        else:\n",
    "            input_size = len(feature_cols)\n",
    "            \n",
    "        self.model = EnhancedBitcoinPredictor(input_size=input_size)\n",
    "        \n",
    "        if model_path and os.path.exists(model_path):\n",
    "            self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=1e-5)\n",
    "        self.buffer = deque(maxlen=1000)\n",
    "        self.steps = 0\n",
    "        self.feature_cols = feature_cols\n",
    "        \n",
    "    def update(self, new_data: pd.DataFrame):\n",
    "        \"\"\"Update model with new data\"\"\"\n",
    "        # Preprocess new data\n",
    "        dataset = BTCDataset()\n",
    "        dataset.feature_cols = self.feature_cols\n",
    "        X, y = dataset.load_dataset()\n",
    "        \n",
    "        # Add to buffer\n",
    "        for i in range(len(X)):\n",
    "            self.buffer.append((\n",
    "                torch.FloatTensor(X[i:i+1]),\n",
    "                torch.FloatTensor(y[i:i+1])\n",
    "            ))\n",
    "        \n",
    "        # Online training step\n",
    "        if len(self.buffer) >= 32:  # Minimum batch size\n",
    "            self.online_train()\n",
    "            \n",
    "    def online_train(self):\n",
    "        self.model.train()\n",
    "        \n",
    "        # Sample batch\n",
    "        batch = random.sample(self.buffer, min(32, len(self.buffer)))\n",
    "        batch_X = torch.cat([x for x, _ in batch]).to(self.device)\n",
    "        batch_y = torch.cat([y for _, y in batch]).to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        point_pred, interval_pred = self.model(batch_X)\n",
    "        \n",
    "        # Loss calculation\n",
    "        loss = challenge_loss(point_pred, interval_pred, batch_y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.steps += 1\n",
    "        \n",
    "        # Periodic validation\n",
    "        if self.steps % 100 == 0:\n",
    "            self.validate()\n",
    "    \n",
    "    def validate(self):\n",
    "        \"\"\"Validate on recent data\"\"\"\n",
    "        if len(self.buffer) < 10:\n",
    "            return\n",
    "            \n",
    "        # Use recent data for validation\n",
    "        val_data = list(self.buffer)[-100:]  # Last 100 samples\n",
    "        val_X = torch.cat([x for x, _ in val_data]).to(self.device)\n",
    "        val_y = torch.cat([y for _, y in val_data]).to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            point_pred, interval_pred = self.model(val_X)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            loss = challenge_loss(point_pred, interval_pred, val_y)\n",
    "            \n",
    "        self.model.train()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on new data\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X).to(self.device)\n",
    "            point_pred, interval_pred = self.model(X_tensor)\n",
    "            return point_pred.cpu().numpy(), interval_pred.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial) -> float:\n",
    "    params = {\n",
    "        'hidden_size': trial.suggest_categorical('hidden_size', [128, 256, 512]),\n",
    "        'num_layers': trial.suggest_int('num_layers', 1, 4),\n",
    "        'lr': trial.suggest_float('lr', 1e-5, 1e-3, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),\n",
    "        'epochs': 2  # Reduced for faster optimization\n",
    "    }\n",
    "    \n",
    "    trial_name = f\"Trial_{trial.number}\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"\\n{'*'*80}\")\n",
    "        print(f\"STARTING {trial_name}\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        print(f\"{'*'*80}\")\n",
    "        \n",
    "        # Load data\n",
    "        print(f\"Loading dataset...\")\n",
    "        try:\n",
    "            dataset = BTCDataset(dataset_path='datasets/structured_dataset.csv')\n",
    "            print(f\"Dataset Found successfully. Loading dataset...\")\n",
    "            X, y = dataset.load_dataset()\n",
    "            \n",
    "            print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "            print(f\"X has {np.isnan(X).sum()} NaN\")\n",
    "            print(f\"y has {np.isnan(y).sum()} NaN\")\n",
    "            assert not np.isnan(X).any(), \"X has NaN\"\n",
    "            assert not np.isnan(y).any(), \"y has NaN\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load dataset: {str(e)}\")\n",
    "            raise\n",
    "        # dataset = BTCDataset(dataset_path='datasets/structured_dataset.csv')\n",
    "        # X, y = dataset.X, dataset.y\n",
    "        # print(f\"Dataset loaded: X shape {X.shape}, y shape {y.shape}\")\n",
    "        \n",
    "        # Train with CV\n",
    "        cv_score = train_with_cv(X, y, params, trial_name=trial_name)\n",
    "        \n",
    "        print(f\"{trial_name} completed successfully with score: {cv_score:.4f}\")\n",
    "        return float(cv_score)\n",
    "    except Exception as e:\n",
    "        print(f\"{trial_name} FAILED with error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return float('inf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-08 17:57:10,455] A new study created in memory with name: no-name-451ff38e-3043-470a-a9d3-ab4a9a6a29dc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization...\n",
      "Target: 100 trials with 24-hour timeout\n",
      "Each trial: 2 epochs, 5-fold CV\n",
      "\n",
      "********************************************************************************\n",
      "STARTING Trial_0\n",
      "Parameters: {'hidden_size': 128, 'num_layers': 3, 'lr': 2.983375321756469e-05, 'batch_size': 32, 'weight_decay': 1.3105819048823466e-06, 'epochs': 2}\n",
      "********************************************************************************\n",
      "Loading dataset...\n",
      "Dataset Found successfully. Loading dataset...\n",
      "Cleaning data...\n",
      "Adding features...\n",
      "Creating sequences...\n",
      "X shape: (976, 24, 25), y shape: (976, 3)\n",
      "X has 0 NaN\n",
      "y has 0 NaN\n",
      "\n",
      "============================================================\n",
      "TRIAL: Trial_0\n",
      "Parameters: {'hidden_size': 128, 'num_layers': 3, 'lr': 2.983375321756469e-05, 'batch_size': 32, 'weight_decay': 1.3105819048823466e-06, 'epochs': 2}\n",
      "============================================================\n",
      "\n",
      "--- Training fold 1/5 ---\n",
      "Train samples: 166, Val samples: 162\n",
      "Starting training for 2 epochs...\n",
      "Batch size: 32, Learning rate: 0.000030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9743/3884307517.py\", line 40, in objective\n",
      "    cv_score = train_with_cv(X, y, params, trial_name=trial_name)\n",
      "  File \"/tmp/ipykernel_9743/271839665.py\", line 74, in train_with_cv\n",
      "    loss = challenge_loss(point_pred, interval_pred, batch_y)\n",
      "  File \"/tmp/ipykernel_9743/3023714606.py\", line 8, in challenge_loss\n",
      "    price_target = targets[:, :, 0]  # price_1h\n",
      "IndexError: too many indices for tensor of dimension 2\n",
      "[I 2025-07-08 17:58:37,104] Trial 0 finished with value: inf and parameters: {'hidden_size': 128, 'num_layers': 3, 'lr': 2.983375321756469e-05, 'batch_size': 32, 'weight_decay': 1.3105819048823466e-06}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial_0 FAILED with error: too many indices for tensor of dimension 2\n",
      "\n",
      "################################################################################\n",
      "OPTIMIZATION PROGRESS: Trial 1/100\n",
      "Best score so far: inf\n",
      "Best trial number: 0\n",
      "################################################################################\n",
      "\n",
      "********************************************************************************\n",
      "STARTING Trial_1\n",
      "Parameters: {'hidden_size': 256, 'num_layers': 1, 'lr': 0.00010398796518457074, 'batch_size': 128, 'weight_decay': 9.39006348184886e-05, 'epochs': 2}\n",
      "********************************************************************************\n",
      "Loading dataset...\n",
      "Dataset Found successfully. Loading dataset...\n",
      "Cleaning data...\n",
      "Adding features...\n",
      "Creating sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9743/3884307517.py\", line 40, in objective\n",
      "    cv_score = train_with_cv(X, y, params, trial_name=trial_name)\n",
      "  File \"/tmp/ipykernel_9743/271839665.py\", line 74, in train_with_cv\n",
      "    loss = challenge_loss(point_pred, interval_pred, batch_y)\n",
      "  File \"/tmp/ipykernel_9743/3023714606.py\", line 8, in challenge_loss\n",
      "    price_target = targets[:, :, 0]  # price_1h\n",
      "IndexError: too many indices for tensor of dimension 2\n",
      "[I 2025-07-08 17:59:59,140] Trial 1 finished with value: inf and parameters: {'hidden_size': 256, 'num_layers': 1, 'lr': 0.00010398796518457074, 'batch_size': 128, 'weight_decay': 9.39006348184886e-05}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (976, 24, 25), y shape: (976, 3)\n",
      "X has 0 NaN\n",
      "y has 0 NaN\n",
      "\n",
      "============================================================\n",
      "TRIAL: Trial_1\n",
      "Parameters: {'hidden_size': 256, 'num_layers': 1, 'lr': 0.00010398796518457074, 'batch_size': 128, 'weight_decay': 9.39006348184886e-05, 'epochs': 2}\n",
      "============================================================\n",
      "\n",
      "--- Training fold 1/5 ---\n",
      "Train samples: 166, Val samples: 162\n",
      "Starting training for 2 epochs...\n",
      "Batch size: 128, Learning rate: 0.000104\n",
      "Trial_1 FAILED with error: too many indices for tensor of dimension 2\n",
      "\n",
      "################################################################################\n",
      "OPTIMIZATION PROGRESS: Trial 2/100\n",
      "Best score so far: inf\n",
      "Best trial number: 0\n",
      "################################################################################\n",
      "\n",
      "********************************************************************************\n",
      "STARTING Trial_2\n",
      "Parameters: {'hidden_size': 256, 'num_layers': 1, 'lr': 0.0008868947791433435, 'batch_size': 32, 'weight_decay': 3.237439857267279e-05, 'epochs': 2}\n",
      "********************************************************************************\n",
      "Loading dataset...\n",
      "Dataset Found successfully. Loading dataset...\n",
      "Cleaning data...\n",
      "Adding features...\n",
      "Creating sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9743/3884307517.py\", line 40, in objective\n",
      "    cv_score = train_with_cv(X, y, params, trial_name=trial_name)\n",
      "  File \"/tmp/ipykernel_9743/271839665.py\", line 74, in train_with_cv\n",
      "    loss = challenge_loss(point_pred, interval_pred, batch_y)\n",
      "  File \"/tmp/ipykernel_9743/3023714606.py\", line 8, in challenge_loss\n",
      "    price_target = targets[:, :, 0]  # price_1h\n",
      "IndexError: too many indices for tensor of dimension 2\n",
      "[I 2025-07-08 18:01:23,675] Trial 2 finished with value: inf and parameters: {'hidden_size': 256, 'num_layers': 1, 'lr': 0.0008868947791433435, 'batch_size': 32, 'weight_decay': 3.237439857267279e-05}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (976, 24, 25), y shape: (976, 3)\n",
      "X has 0 NaN\n",
      "y has 0 NaN\n",
      "\n",
      "============================================================\n",
      "TRIAL: Trial_2\n",
      "Parameters: {'hidden_size': 256, 'num_layers': 1, 'lr': 0.0008868947791433435, 'batch_size': 32, 'weight_decay': 3.237439857267279e-05, 'epochs': 2}\n",
      "============================================================\n",
      "\n",
      "--- Training fold 1/5 ---\n",
      "Train samples: 166, Val samples: 162\n",
      "Starting training for 2 epochs...\n",
      "Batch size: 32, Learning rate: 0.000887\n",
      "Trial_2 FAILED with error: too many indices for tensor of dimension 2\n",
      "\n",
      "################################################################################\n",
      "OPTIMIZATION PROGRESS: Trial 3/100\n",
      "Best score so far: inf\n",
      "Best trial number: 0\n",
      "################################################################################\n",
      "\n",
      "********************************************************************************\n",
      "STARTING Trial_3\n",
      "Parameters: {'hidden_size': 512, 'num_layers': 3, 'lr': 0.00027356072515645165, 'batch_size': 32, 'weight_decay': 0.0001997482414600663, 'epochs': 2}\n",
      "********************************************************************************\n",
      "Loading dataset...\n",
      "Dataset Found successfully. Loading dataset...\n",
      "Cleaning data...\n",
      "Adding features...\n",
      "Creating sequences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9743/3884307517.py\", line 40, in objective\n",
      "    cv_score = train_with_cv(X, y, params, trial_name=trial_name)\n",
      "  File \"/tmp/ipykernel_9743/271839665.py\", line 74, in train_with_cv\n",
      "    loss = challenge_loss(point_pred, interval_pred, batch_y)\n",
      "  File \"/tmp/ipykernel_9743/3023714606.py\", line 8, in challenge_loss\n",
      "    price_target = targets[:, :, 0]  # price_1h\n",
      "IndexError: too many indices for tensor of dimension 2\n",
      "[I 2025-07-08 18:02:47,874] Trial 3 finished with value: inf and parameters: {'hidden_size': 512, 'num_layers': 3, 'lr': 0.00027356072515645165, 'batch_size': 32, 'weight_decay': 0.0001997482414600663}. Best is trial 0 with value: inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (976, 24, 25), y shape: (976, 3)\n",
      "X has 0 NaN\n",
      "y has 0 NaN\n",
      "\n",
      "============================================================\n",
      "TRIAL: Trial_3\n",
      "Parameters: {'hidden_size': 512, 'num_layers': 3, 'lr': 0.00027356072515645165, 'batch_size': 32, 'weight_decay': 0.0001997482414600663, 'epochs': 2}\n",
      "============================================================\n",
      "\n",
      "--- Training fold 1/5 ---\n",
      "Train samples: 166, Val samples: 162\n",
      "Starting training for 2 epochs...\n",
      "Batch size: 32, Learning rate: 0.000274\n",
      "Trial_3 FAILED with error: too many indices for tensor of dimension 2\n",
      "\n",
      "################################################################################\n",
      "OPTIMIZATION PROGRESS: Trial 4/100\n",
      "Best score so far: inf\n",
      "Best trial number: 0\n",
      "################################################################################\n",
      "\n",
      "********************************************************************************\n",
      "STARTING Trial_4\n",
      "Parameters: {'hidden_size': 256, 'num_layers': 3, 'lr': 3.025783450424705e-05, 'batch_size': 64, 'weight_decay': 6.789817585646837e-06, 'epochs': 2}\n",
      "********************************************************************************\n",
      "Loading dataset...\n",
      "Dataset Found successfully. Loading dataset...\n",
      "Cleaning data...\n",
      "Adding features...\n",
      "Creating sequences...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if dataset exists\n",
    "    if not os.path.exists('datasets/structured_dataset.csv'):\n",
    "        print(\"Dataset not found. Please ensure 'datasets/structured_dataset.csv' exists.\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"Starting hyperparameter optimization...\")\n",
    "    print(f\"Target: 100 trials with 24-hour timeout\")\n",
    "    print(f\"Each trial: 2 epochs, 5-fold CV\")\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    # Add a callback to show progress\n",
    "    def progress_callback(study, trial):\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"OPTIMIZATION PROGRESS: Trial {trial.number + 1}/100\")\n",
    "        print(f\"Best score so far: {study.best_value:.4f}\")\n",
    "        print(f\"Best trial number: {study.best_trial.number}\")\n",
    "        print(f\"{'#'*80}\")\n",
    "    \n",
    "    study.optimize(objective, n_trials=10, timeout=24*60*60, callbacks=[progress_callback])\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"HYPERPARAMETER OPTIMIZATION COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Trial Number: {trial.number}\")\n",
    "    print(f\"  CV Score: {trial.value}\")\n",
    "    print(\"  Best Parameters: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Save best parameters\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    with open('models/best_params.json', 'w') as f:\n",
    "        json.dump(trial.params, f, indent=2)\n",
    "    \n",
    "    print(\"\\nTraining final model with best parameters...\")\n",
    "    # Train final model with best parameters\n",
    "    dataset = BTCDataset(dataset_path='datasets/structured_dataset.csv')\n",
    "    X, y = dataset.load_dataset()\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    # check NaN in X, y\n",
    "    print(f\"X has {np.isnan(X).sum()} NaN\")\n",
    "    print(f\"y has {np.isnan(y).sum()} NaN\")\n",
    "    assert not np.isnan(X).any(), \"X has NaN\"\n",
    "    assert not np.isnan(y).any(), \"y has NaN\"\n",
    "    final_score = train_with_cv(X, y, trial.params, trial_name=\"FINAL_MODEL\")\n",
    "    print(f\"Final CV Score: {final_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prec-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
