training:
  batch_size: 64
  epochs: 15
  lr_ramp_epochs: 3
  early_stopping_patience: 5

model:
  ts_hidden_size: 64
  num_adapters: 4
  adapter_size: 128
  dropout: 0.1

optimizer:
  point_weight: 0.65
  max_lr: 1e-3
  weight_decay: 0.01