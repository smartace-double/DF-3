# LSTM Bitcoin Price Predictor Configuration - Precog Mode
# This configuration is optimized for the precog subnet challenge format
# Predicts point and interval estimates for 1-hour ahead Bitcoin prices

# Model Configuration
model_type: LSTM
mode: precog

# Model Architecture
hidden_size: 256
num_layers: 3
dropout: 0.1
activation: SiLU
use_layer_norm: true
bidirectional: false

# Training Configuration
lr: 0.0001
batch_size: 512
epochs: 10
weight_decay: 0.00001
grad_clip: 1.0
patience: 5

# Data Configuration
dataset_path: datasets/complete_dataset_20250709_152829.csv
lookback: 72        # 6 hours * 12 (5-minute intervals)
horizon: 12         # 1 hour * 12 (5-minute intervals)
train_split: 0.85
val_split: 0.10
test_split: 0.05

# System Configuration
save_dir: models
use_gpu: true
mixed_precision: true
num_workers: 4
pin_memory: true
persistent_workers: true

# Optuna Hyperparameter Optimization Configuration
optuna:
  # Study Configuration
  study_name: lstm_precog_optimization
  n_trials: 100
  timeout: 86400  # 24 hours in seconds
  n_cv_folds: 5
  
  # Architecture Search Space
  architecture:
    hidden_size: [64, 128, 256, 512]
    num_layers: [1, 2, 3, 4]
    dropout: [0.0, 0.5]
    bidirectional: [true, false]
    use_layer_norm: [true, false]
    activation: [SiLU, GELU, ReLU, Tanh]
  
  # Training Search Space
  training:
    epochs: [10, 100]
    batch_size: [256, 512, 1024, 2048]
    grad_clip: [0.1, 2.0]
    patience: [5, 20]
    mixed_precision: [true, false]
  
  # Optimizer Search Space
  optimizer:
    type: [AdamW, Adam, RMSprop, SGD]
    lr: [1e-5, 1e-2]  # log scale
    weight_decay: [1e-6, 1e-3]  # log scale
    beta1: [0.8, 0.99]  # for Adam/AdamW
    beta2: [0.9, 0.999]  # for Adam/AdamW
    eps: [1e-8, 1e-6]  # log scale
    momentum: [0.0, 0.9]  # for SGD/RMSprop
  
  # Scheduler Search Space
  scheduler:
    type: [OneCycleLR, CosineAnnealingLR, ReduceLROnPlateau, StepLR, None]
    pct_start: [0.1, 0.5]  # OneCycleLR
    div_factor: [10.0, 100.0]  # OneCycleLR
    final_div_factor: [100.0, 10000.0]  # OneCycleLR
    T_max: [10, 100]  # CosineAnnealingLR
    eta_min: [1e-6, 1e-4]  # CosineAnnealingLR, log scale
    factor: [0.1, 0.8]  # ReduceLROnPlateau
    step_size: [10, 50]  # StepLR
    gamma: [0.1, 0.9]  # StepLR
  
  # Preprocessing Search Space
  preprocessing:
    scaler_type: [standard, robust, minmax]
    lookback: [48, 72, 96, 120]
    handle_outliers: [true, false]
    add_technical_features: [true, false] 